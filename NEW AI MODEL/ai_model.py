# -*- coding: utf-8 -*-
"""AI MODEL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N2ahxhjfKQObZ0VcqpR6k-tsxtF4mXSy
"""

pip install torch torchvision scikit-learn opencv-python tqdm

from google.colab import drive
drive.mount('/content/drive')

import os

file_path = "/content/drive/MyDrive/New folder.zip"
if os.path.exists(file_path):
    print("âœ… ZIP file mil gayi:", file_path)
else:
    print("âŒ ZIP file nahi mili:", file_path)

!unzip "/content/drive/MyDrive/New folder.zip" -d "/content/sample_data"

import torch
import torchvision.models as models

model = models.resnet50(pretrained=True)
model.eval()
feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])

import torchvision.transforms as transforms
import cv2

transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def preprocess_image(img_path):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    return transform(img).unsqueeze(0)

import numpy as np

def get_feature_vector(img_path):
    img_tensor = preprocess_image(img_path)
    with torch.no_grad():
        features = feature_extractor(img_tensor).squeeze().numpy()
    return features / np.linalg.norm(features)

from sklearn.metrics.pairwise import cosine_similarity

def pair_images(image_paths, features):
    pairs = []
    used = set()
    for i, img1 in enumerate(image_paths):
        if img1 in used:
            continue
        best_match = None
        best_score = 0
        for j, img2 in enumerate(image_paths):
            if img1 == img2 or img2 in used:
                continue
            sim = cosine_similarity([features[img1]], [features[img2]])[0][0]
            if sim > best_score:
                best_score = sim
                best_match = img2
        if best_match:
            pairs.append((img1, best_match))
            used.add(img1)
            used.add(best_match)
    return pairs

def classify_room(image_path):
    name = image_path.lower()
    if "kitchen" in name:
        return "Kitchen"
    elif "living" in name:
        return "Living Room"
    elif "closet" in name:
        return "Closet"
    else:
        return "Room"

import os
import shutil

def save_pairs(pairs, output_folder):
    os.makedirs(output_folder, exist_ok=True)
    for i, (before, after) in enumerate(pairs):
        label = classify_room(before + after)
        room_folder = os.path.join(output_folder, label)
        os.makedirs(room_folder, exist_ok=True)
        shutil.copy(before, os.path.join(room_folder, f"{label.lower()}_{i}_before.jpg"))
        shutil.copy(after, os.path.join(room_folder, f"{label.lower()}_{i}_after.jpg"))

from tqdm import tqdm

def organize_images(input_folder, output_folder):
    image_paths = []
    for root, dirs, files in os.walk(input_folder):
        for file in files:
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                image_paths.append(os.path.join(root, file))

    print(f"âœ… Found {len(image_paths)} images.")
    print("ðŸ“¸ First few images:", image_paths[:5])

    features = {img: get_feature_vector(img) for img in tqdm(image_paths, desc="Extracting Features")}
    pairs = pair_images(image_paths, features)
    save_pairs(pairs, output_folder)

organize_images("/content/sample_data/New folder", "/content/Organized_Project")

!pip install timm --quiet

import torch
import torchvision.transforms as transforms
from PIL import Image
import timm
import json
import urllib.request

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = timm.create_model('convnext_tiny.fb_in22k', pretrained=True)
model.eval().to(device)

# Load class index labels
url = 'https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json'
with urllib.request.urlopen(url) as f:
    idx_to_labels = json.load(f)

ROOM_LABELS = {
    "kitchen": ["kitchen", "oven", "stove", "microwave"],
    "living room": ["living", "tv", "sofa", "couch"],
    "closet": ["closet", "wardrobe", "clothes"],
    "bathroom": ["bathroom", "toilet", "sink", "shower"],
    "bedroom": ["bed", "pillow"],
}

def predict_room_type(image_path):
    try:
        image = Image.open(image_path).convert('RGB')
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
        ])
        img_tensor = transform(image).unsqueeze(0).to(device)
        with torch.no_grad():
            outputs = model(img_tensor)
        _, predicted = outputs.max(1)
        label_text = idx_to_labels[str(predicted.item())][1].lower()
        for room, keywords in ROOM_LABELS.items():
            if any(keyword in label_text for keyword in keywords):
                return room.capitalize()
        return "Other"
    except:
        return "Other"

def save_pairs(pairs, output_folder):
    os.makedirs(output_folder, exist_ok=True)
    for i, (before, after) in enumerate(pairs):
        room_type = predict_room_type(before)
        room_folder = os.path.join(output_folder, room_type)
        os.makedirs(room_folder, exist_ok=True)
        shutil.copy(before, os.path.join(room_folder, f"{room_type.lower()}_{i}_before.jpg"))
        shutil.copy(after, os.path.join(room_folder, f"{room_type.lower()}_{i}_after.jpg"))

organize_images("/content/sample_data/New folder", "/content/Organized_Project")

import torch
import torchvision.transforms as transforms
from PIL import Image
from torchvision import models

# Load a pre-trained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet18(pretrained=True)
model = model.to(device)
model.eval()

# Dummy room labels â€“ You can improve this by fine-tuning
room_keywords = {
    "kitchen": ["kitchen", "microwave", "oven", "refrigerator"],
    "bedroom": ["bed", "pillow", "blanket"],
    "bathroom": ["toilet", "sink", "bath"],
    "living_room": ["sofa", "television", "lamp"],
    "other": []
}

def classify_room_type(image_path):
    input_image = Image.open(image_path).convert("RGB")
    preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor()
    ])
    input_tensor = preprocess(input_image).unsqueeze(0).to(device)

    with torch.no_grad():
        outputs = model(input_tensor)
        _, predicted = torch.max(outputs, 1)

    # Get class label from ImageNet
    class_idx = predicted.item()
    label = models.detection.IMAGE_NET_CLASSES[class_idx].lower()

    for room, keywords in room_keywords.items():
        if any(keyword in label for keyword in keywords):
            return room
    return "other"

def guess_room_type(image_path):
    filename = os.path.basename(image_path).lower()

    if "kitchen" in filename or "031" in filename:
        return "Kitchen"
    elif "living" in filename or "032" in filename:
        return "Living Room"
    elif "bed" in filename or "745" in filename:
        return "Bedroom"
    else:
        return "Unknown"

def save_pairs_by_room_type(pairs, output_folder):
    os.makedirs(output_folder, exist_ok=True)

    for before_path, after_path in pairs:
        room_type = guess_room_type(before_path)
        room_folder = os.path.join(output_folder, room_type)
        os.makedirs(room_folder, exist_ok=True)

        # Copy files with proper naming
        shutil.copy2(before_path, os.path.join(room_folder, f"before_{os.path.basename(before_path)}"))
        shutil.copy2(after_path, os.path.join(room_folder, f"after_{os.path.basename(after_path)}"))

    print("âœ… Images saved by room type.")

# âœ… Call this after pairing
save_pairs_by_room_type(pairs, "/content/Organized_Project")

import os
import json

output_folder = "/content/Organized_Project"

# Dictionary to hold final result
organized_data = {}

# Loop through subfolders like Kitchen, Living Room, etc.
for room_type in os.listdir(output_folder):
    room_path = os.path.join(output_folder, room_type)
    if os.path.isdir(room_path):
        files = sorted(os.listdir(room_path))
        before_img = [f for f in files if 'before' in f.lower()]
        after_img = [f for f in files if 'after' in f.lower()]

        # Pair up images and store
        pairs = list(zip(before_img, after_img))
        organized_data[room_type] = []

        for b, a in pairs:
            organized_data[room_type].append({
                "before": b,
                "after": a,
                "title": room_type
            })

# Save the JSON file
json_path = os.path.join(output_folder, "image_pairs_by_room.json")
with open(json_path, "w") as f:
    json.dump(organized_data, f, indent=4)

print(f"âœ… JSON saved to: {json_path}")